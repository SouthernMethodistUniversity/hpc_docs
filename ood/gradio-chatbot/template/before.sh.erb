# Find available port to run server on
port=$(find_port ${host})

export GRADIO_SERVER_PORT=${port}
export GRADIO_SERVER_NAME="0.0.0.0"
export GRADIO_DEBUG=1
export GRADIO_ROOT_PATH="/node/${host}/${port}"
export FORWARDED_ALLOW_IPS="127.0.0.1"

################################################################################

export APP="${PWD}/app.py"
export HF_HOME="${SCRATCH}/${SLURM_JOB_ID}"
export MODEL="<%= context.model =%>"


################################################################################

(
umask 077
cat > "$APP" << EOL
#!/usr/bin/env python3

import llama_cpp
import llama_cpp.llama_tokenizer
import gradio as gr

llama = llama_cpp.Llama(model_path="$MODEL", n_gpu_layers=-1, context=2048, verbose=False)

def predict(message, history):
    messages = []
    for user_message, assistant_message in history:
        messages.append({"role": "user", "content": user_message})
        messages.append({"role": "assistant", "content": assistant_message})
    messages.append({"role": "user", "content": message})

    response = llama.create_chat_completion(messages=messages, stream=True)

    text = ""
    for chunk in response:
        content = chunk.choices[0].delta.content
        if content:
            text += content
            yield text

with gr.Blocks(theme=gr.themes.Soft(), fill_height=True, head='<link rel="icon" href="/favicon.ico">') as demo:
    gr.ChatInterface(
        predict,
        fill_height=True,
        examples=[
            "What year is it?",
            "Where is Southern Methodist University?",
            "What is Open OnDemand?",
            "How many r's are in the word strawberry?",
        ],
    )

if __name__ == "__main__":
    demo.launch()
EOL
)
